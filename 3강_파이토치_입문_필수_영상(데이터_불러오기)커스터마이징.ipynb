{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3강. 파이토치 입문 필수 영상(데이터 불러오기)커스터마이징.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMIs7MDbMWAUgDi8Y976ZEM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/studian/study_pytorch/blob/main/3%EA%B0%95_%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98_%EC%9E%85%EB%AC%B8_%ED%95%84%EC%88%98_%EC%98%81%EC%83%81(%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0)%EC%BB%A4%EC%8A%A4%ED%84%B0%EB%A7%88%EC%9D%B4%EC%A7%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1PRu9MUvaZ2"
      },
      "source": [
        "**1. 파이토치 제공 데이터 사용**\r\n",
        "\r\n",
        "**2. 같은 클래스 별 폴더 이미지 데이터 이용**\r\n",
        "\r\n",
        "**3. 개인 데이터 사용 (2types)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTu-_1iIvk1b"
      },
      "source": [
        "import torch\r\n",
        "import torchvision"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKylLn-swKLD"
      },
      "source": [
        "# 데이터 불러와서 전처리 바로 할때 사용\r\n",
        "import torchvision.transforms as tr \r\n",
        "# DataLoader: 학습시 batch 크기만큼 데이터를 불러와주는 역할을해줌\r\n",
        "# Dataset: \r\n",
        "from torch.utils.data import DataLoader, Dataset \r\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN-dfE-Uw18v"
      },
      "source": [
        "**1. 파이토치 제공 데이터 사용**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCHLWviSx209"
      },
      "source": [
        "# 전처리는 tr.Compose()를 사용하고, 전처리 순서는 # tr.Compose()안의 [함수1, 함수2, ... 함수3]의 순서로 진행됨\r\n",
        "# All transformations accept PIL Image: PIL Image 형태만 됨, numpy 나 다른 데이터 형태, 즉. PIL Image 아니면 에러\r\n",
        "# Pad, Grayscale, RandomCrop, Normalize, ..\r\n",
        "# Transforms on torch.*Tensor - tensor image\r\n",
        "# torchvision.transform.ToPILImage(mode=Nome)...\r\n",
        "# ...\r\n",
        "transf = tr.Compose([tr.Resize(8), tr.ToTensor()]) # 8x8로 리사이즈 -> tensor 데이터로 바꿔줌"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaSfHhZvwdNU",
        "outputId": "2d157e65-620a-4324-b8d8-20ec68cb1321"
      },
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transf)\r\n",
        "len(trainset) # 학습 데이터 셋 개수\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_pz28_z1JI7",
        "outputId": "e06fbcf0-68e5-424d-a617-b023e34b30ee"
      },
      "source": [
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transf)\r\n",
        "len(testset) # 테스트 데이터 셋 개수"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEkBelEtxXxw",
        "outputId": "1921dd2c-912d-48b8-cc7f-571ad8576575"
      },
      "source": [
        "# trainset의 크기 보기: [이미지 채널, 이미지 가로, 이미지 세로] = [3, 8, 8]: pytorch는 채널수가 앞쪽에 있음\r\n",
        "trainset[0][0].size()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 8, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtFrIJYqxX04"
      },
      "source": [
        "trainLoader = DataLoader(trainset, batch_size=50, shuffle=True, num_workers=2) # num_worker: 데이터 로드 시 가용 스레드? 여튼 작업자 개수, 에러뜨면 0으로 세팅하면됨\r\n",
        "testLoader = DataLoader(testset, batch_size=50, shuffle=True, num_workers=2) # num_worker: 데이터 로드 시 가용 스레드? 여튼 작업자 개수, 에러뜨면 0으로 세팅하면됨"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GnyqS3V0uCq",
        "outputId": "91e8e5d3-ce03-4834-ecad-3ccf94e3e632"
      },
      "source": [
        "# 학습데이터: 5만개(50000), 배치크기: 50 -> 50000/50 = 1000 --> 즉, 1000번 데이터 로드 해야함: len(trainLoader) = 1000 \r\n",
        "len(trainLoader) # 1000: "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taYSd3Nr0uKL"
      },
      "source": [
        "data_iter = iter(trainLoader)\r\n",
        "images, labels = data_iter.next()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4UdG-Ox0uNC",
        "outputId": "27a713f2-04f6-46da-e752-f0ffd1dac3aa"
      },
      "source": [
        "images.size() # torch.Size([50, 3, 8, 8]): 배치사이즈 50개, 3채널, 이미지 가로 8, 이미지 세로 8"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 3, 8, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnrBvD_ExCj9"
      },
      "source": [
        "**2. 같은 클래스 별 폴더 이미지 데이터 이용**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_idfhVT_3s3u"
      },
      "source": [
        "```python\r\n",
        "# 데이터가 ./class/tiger  ./class/lion <- 이렇게 2개 폴더 내에 이미지가 있다고 가정\r\n",
        "\r\n",
        "transf = tr.Compose([tr.Resize(16), tr.ToTensor()]) # 전처리 함수들 등록\r\n",
        "trainset = torchvision.datasets.ImageFolder(root='./class', train=True, transform=transf) # ./class 폴더의 하위 폴더들이 자동으로 라벨링 되면서 데이터셋이 만들어짐\r\n",
        "trainLoader = DataLoader(trainset, batch_size=10, shuffle=False, num_workers=2) # num_worker: 데이터 로드 시 가용 스레드? 여튼 작업자 개수, 에러뜨면 0으로 세팅하면됨\r\n",
        "print(len(trainLoader))\r\n",
        "trainset[0][0].size()\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw78O9qixCsT"
      },
      "source": [
        "**3. 개인 데이터 사용 (2types)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd0ekZss8JPq"
      },
      "source": [
        "**종류1 ==> torchvision.datasets.ImageFolder 사용 안함**\r\n",
        "왜? \r\n",
        "\r\n",
        "1. 폴더 정리를 못하는 경우\r\n",
        "1) 다른 작업과 공용으로 사용해서 하위폴더 정리를 못한다던가, 이름을 못바꾼다던가 할때\r\n",
        "2) 폴더가 아닌 SQL 같은곳에서 데이터가 넘어오는 경우\r\n",
        "\r\n",
        "2. tr.Compose() 함수에서 제공하는 전처리 함수들의 개수가 너무 작다.\r\n",
        "1) OpenCV 가 훨씬 많고, 아니면, \r\n",
        "2) 내가 직접 preprocessing() 만들어서 사용하는 경우.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbGKMNSnxK6c",
        "outputId": "8313b866-e62c-4ee7-d862-6dfd35c7740c"
      },
      "source": [
        "#import preprocessing\r\n",
        "# 이미지가 32x32 크기의 3채널 이미지, 20개, 그에 따른 라벨 20개가 numpy형태로 로드 됐다고 가정하자.\r\n",
        "\r\n",
        "train_images = np.random.randint(256, size=(20,32,32,3))    # 0~255의 정수 값을 랜덤으로 가짐\r\n",
        "train_labels = np.random.randint(2, size=(20,1))            # 0~1의 정수 값을 랜덤으로 가짐\r\n",
        "\r\n",
        "# preprocessing ....\r\n",
        "# train_images, train_labels = preprocessing(train_imges, train_labels)\r\n",
        "\r\n",
        "print(train_images.shape, train_labels.shape)\r\n",
        "# 이미지: (20, 32, 32, 3)\r\n",
        "#  index:   0   1   2  3\r\n",
        "#         개수, 가로, 세로, 채널"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20, 32, 32, 3) (20, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MyABrVuxZAu"
      },
      "source": [
        "class TensorData(Dataset):\r\n",
        "  def __init__(self, x_data, y_data):\r\n",
        "    self.x_data = torch.FloatTensor(x_data)\r\n",
        "    self.x_data = self.x_data.permute(0,3,1,2) ## 이미지 개수, 채널수, 이미지 가로, 이미지 세로: permute() 함수로 순서 바꿔줌\r\n",
        "    self.y_data = torch.LongTensor(y_data)\r\n",
        "    self.len = self.y_data.shape[0]\r\n",
        "\r\n",
        "  def __getitem__(self, index): # tuple 형태로 바꿔줌\r\n",
        "    return self.x_data[index], self.y_data[index]\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.len"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccj8EKayxZEC"
      },
      "source": [
        "train_data = TensorData(train_images, train_labels)\r\n",
        "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zczLoUvI7H55",
        "outputId": "26cc6d73-8979-4924-cd93-a13a5bc0fab1"
      },
      "source": [
        "train_data[0][0].size()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIf_8W6l7pVL"
      },
      "source": [
        "data_iter = iter(train_loader)\r\n",
        "images, labels = data_iter.next()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1uPcuTY703A",
        "outputId": "23e57659-af67-42c5-9e54-e3ecd03355b0"
      },
      "source": [
        "images.size() # torch.Size([10, 3, 32, 32]): 배치사이즈 10개, 3채널, 이미지 가로 32, 이미지 세로 32"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 32, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISUZgXFa9h4V"
      },
      "source": [
        "**개 꿀팁 방출**\r\n",
        "\r\n",
        "기본 틀\r\n",
        "```python\r\n",
        "from torch.utils.data import Dataset\r\n",
        "\r\n",
        "class MyDataset(Dataset):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "\r\n",
        "  def __getitem__(self, index): # tuple 형태로 바꿔줌\r\n",
        "\r\n",
        "  def __len__(self):  \r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLZOG8BwIWui"
      },
      "source": [
        "**1. ToTensor 조차도 내가 만들자. 즉 PIL image 형태로 값을 받게 바꾸는거 귀찮다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-4DRFli72lv"
      },
      "source": [
        "class MyDataset(Dataset):\r\n",
        "\r\n",
        "  def __init__(self, x_data, y_data, transform=None):\r\n",
        "    # 텐서 변환 안함\r\n",
        "    self.x_data = x_data\r\n",
        "    self.y_data = y_data\r\n",
        "    self.transform = transform\r\n",
        "    self.len = len(y_data)\r\n",
        "    \r\n",
        "  def __getitem__(self, index): \r\n",
        "    # 튜플 형태로 내보내기 전에 전처리 작업 실행\r\n",
        "    sample = self.x_data[index], self.y_data[index]\r\n",
        "    if self.transform:\r\n",
        "      sample = self.transform(sample)\r\n",
        "\r\n",
        "    return sample   # 넘파이로 출력됨\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.len\r\n",
        "\r\n",
        "class MyToTensor:\r\n",
        "  def __call__(self, sample):\r\n",
        "    inputs, labels = sample\r\n",
        "    inputs = torch.FloatTensor(inputs)\r\n",
        "    inputs = inputs.permute(2,0,1)\r\n",
        "    return inputs, torch.LongTensor(labels)\r\n",
        "\r\n",
        "# 들어온 데이터를 연산\r\n",
        "class MyLinearTensor:\r\n",
        "  def __init__(self, slope=1, bias=0):\r\n",
        "    self.slope = slope\r\n",
        "    self.bias = bias\r\n",
        "\r\n",
        "  def __call__(self, sample):\r\n",
        "    inputs, labels = sample\r\n",
        "    inputs = self.slope * inputs + self.bias\r\n",
        "    return inputs, labels\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzVkdqO_AfQU"
      },
      "source": [
        "**사용법**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ThnsuNWAePB"
      },
      "source": [
        "trans = tr.Compose([MyToTensor(),MyLinearTensor(2,5)]) # tr.Compose 지정: 전처리 할것 순서대로 나열\r\n",
        "\r\n",
        "ds1 = MyDataset(train_images, train_labels, transform=trans)\r\n",
        "\r\n",
        "train_loader1 = DataLoader(ds1, batch_size=10, shuffle=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KutXHBFPAq1Z",
        "outputId": "7119deb8-4ed4-4211-8ae8-644298a634df"
      },
      "source": [
        "first_data = ds1[0]\r\n",
        "\r\n",
        "features, labels = first_data\r\n",
        "\r\n",
        "print(type(features), type(labels))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ0WW1I8Bg8y"
      },
      "source": [
        "dataiter1 = iter(train_loader1)\r\n",
        "images1, labels1 = dataiter1.next()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H51Jk1eCQ6K",
        "outputId": "0e3a1ff2-834c-42bd-96c8-82ab31e1b905"
      },
      "source": [
        "# 원래는 이미지가 # 0~255의 정수 값을 가졌으나, \r\n",
        "# LinearTensor(2,5) 연산으로 인해 inputs = self.slope * inputs + self.bias\r\n",
        "# 즉 2*(0~255) + 5의 값으로 변했음. -> (5~515) 값으로 변함\r\n",
        "images1"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 25., 503.,  75.,  ..., 409., 305., 147.],\n",
              "          [375.,  21., 403.,  ...,  45., 263.,  97.],\n",
              "          [275., 203., 297.,  ..., 351.,  87., 129.],\n",
              "          ...,\n",
              "          [235., 303., 451.,  ..., 219., 157., 109.],\n",
              "          [157., 293.,  61.,  ..., 109., 337., 265.],\n",
              "          [449., 513.,  89.,  ..., 221., 423., 411.]],\n",
              "\n",
              "         [[413., 301.,  57.,  ..., 279., 419., 211.],\n",
              "          [195., 225., 239.,  ..., 369., 469., 439.],\n",
              "          [ 51.,  49., 121.,  ..., 119., 107., 259.],\n",
              "          ...,\n",
              "          [191., 485., 429.,  ..., 441., 215., 119.],\n",
              "          [307., 299., 347.,  ..., 183., 481., 201.],\n",
              "          [277., 265., 393.,  ..., 165.,  47., 167.]],\n",
              "\n",
              "         [[215., 333.,   9.,  ..., 263., 265., 417.],\n",
              "          [ 99., 171., 275.,  ..., 341., 123., 127.],\n",
              "          [413., 225., 349.,  ..., 127., 193., 333.],\n",
              "          ...,\n",
              "          [417., 165.,  41.,  ...,  53., 283., 245.],\n",
              "          [ 91.,  27., 237.,  ...,  85.,  71., 451.],\n",
              "          [123., 285., 129.,  ..., 215., 201., 463.]]],\n",
              "\n",
              "\n",
              "        [[[ 43.,  33., 275.,  ..., 217., 273., 277.],\n",
              "          [295., 499., 307.,  ..., 367., 275.,  13.],\n",
              "          [145., 289., 465.,  ..., 175., 213.,  79.],\n",
              "          ...,\n",
              "          [287.,  27., 453.,  ..., 345., 137., 335.],\n",
              "          [ 83.,  85.,  81.,  ..., 209., 421., 251.],\n",
              "          [493., 439., 363.,  ..., 305., 325., 391.]],\n",
              "\n",
              "         [[289., 155., 137.,  ..., 189., 199., 455.],\n",
              "          [163., 471., 211.,  ..., 275., 113., 175.],\n",
              "          [119., 313.,  59.,  ..., 293., 221., 207.],\n",
              "          ...,\n",
              "          [293., 207.,  91.,  ..., 461., 243., 157.],\n",
              "          [165., 223., 219.,  ..., 401., 175., 163.],\n",
              "          [411.,  95., 245.,  ...,  11., 205., 491.]],\n",
              "\n",
              "         [[295.,  55., 441.,  ..., 413., 307.,  27.],\n",
              "          [497., 137., 119.,  ..., 473., 499., 175.],\n",
              "          [ 21., 175., 289.,  ..., 385., 109., 515.],\n",
              "          ...,\n",
              "          [455., 271.,  31.,  ..., 327., 469., 103.],\n",
              "          [ 55., 221., 191.,  ..., 103., 453., 291.],\n",
              "          [161., 229., 499.,  ..., 199., 307.,  45.]]],\n",
              "\n",
              "\n",
              "        [[[261.,  83., 157.,  ..., 141., 197., 409.],\n",
              "          [349.,  71., 491.,  ...,  51., 215.,   9.],\n",
              "          [437., 431.,  23.,  ..., 391., 133.,  61.],\n",
              "          ...,\n",
              "          [497., 391.,  37.,  ...,  17., 343.,  51.],\n",
              "          [311., 437.,  29.,  ..., 259.,  39., 147.],\n",
              "          [367., 379., 193.,  ..., 323., 479., 391.]],\n",
              "\n",
              "         [[309., 465.,  33.,  ..., 265., 255., 365.],\n",
              "          [323., 485., 407.,  ..., 215., 445., 189.],\n",
              "          [ 65., 479., 193.,  ...,  37., 369.,  61.],\n",
              "          ...,\n",
              "          [ 51., 375.,  89.,  ...,  79., 139., 299.],\n",
              "          [455., 487., 243.,  ..., 131., 459.,  97.],\n",
              "          [289., 471.,  57.,  ..., 359., 401.,  15.]],\n",
              "\n",
              "         [[393., 205.,  53.,  ..., 465., 341., 293.],\n",
              "          [411.,  99., 361.,  ..., 149., 139., 141.],\n",
              "          [ 17., 491.,  83.,  ...,  85., 169., 399.],\n",
              "          ...,\n",
              "          [399., 277., 267.,  ..., 251., 435., 247.],\n",
              "          [323., 347., 205.,  ..., 213., 455., 485.],\n",
              "          [473., 371., 463.,  ..., 255., 225., 119.]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[471., 329., 207.,  ..., 281., 325., 465.],\n",
              "          [355., 237., 415.,  ..., 171., 339., 477.],\n",
              "          [ 61., 211., 467.,  ...,  67.,  69., 273.],\n",
              "          ...,\n",
              "          [473., 355., 441.,  ..., 441., 333.,  97.],\n",
              "          [103., 443., 445.,  ..., 263., 439.,  11.],\n",
              "          [ 63., 199., 157.,  ..., 265., 221., 439.]],\n",
              "\n",
              "         [[479., 471., 481.,  ..., 223., 309., 235.],\n",
              "          [ 83., 159., 389.,  ...,  11., 305., 107.],\n",
              "          [357., 355.,   5.,  ...,  75., 205., 421.],\n",
              "          ...,\n",
              "          [ 25., 381., 417.,  ..., 435., 297., 423.],\n",
              "          [245., 353., 299.,  ..., 411., 341., 209.],\n",
              "          [121., 305., 221.,  ..., 395., 177.,  75.]],\n",
              "\n",
              "         [[311., 299.,  17.,  ..., 171.,  99., 471.],\n",
              "          [395., 449.,  63.,  ..., 263., 125., 193.],\n",
              "          [305., 433.,  99.,  ..., 421., 455., 489.],\n",
              "          ...,\n",
              "          [319., 359., 379.,  ...,  17.,  79., 275.],\n",
              "          [439., 115., 403.,  ...,  85., 437., 267.],\n",
              "          [413.,   9., 109.,  ...,  23., 451., 239.]]],\n",
              "\n",
              "\n",
              "        [[[475.,  77., 431.,  ..., 179.,  73.,  37.],\n",
              "          [355., 463., 405.,  ..., 139., 401., 497.],\n",
              "          [389., 477., 151.,  ...,  43., 221., 345.],\n",
              "          ...,\n",
              "          [165., 281., 163.,  ..., 363., 173.,  25.],\n",
              "          [ 91., 477.,  33.,  ..., 431., 387., 315.],\n",
              "          [ 37., 251., 257.,  ..., 339., 239., 111.]],\n",
              "\n",
              "         [[285., 237., 481.,  ..., 101., 507., 329.],\n",
              "          [241., 247., 323.,  ..., 483.,  75., 511.],\n",
              "          [455., 183., 177.,  ..., 153.,  45., 297.],\n",
              "          ...,\n",
              "          [501., 193., 255.,  ...,  27., 425.,  17.],\n",
              "          [405., 279., 413.,  ..., 147., 375., 191.],\n",
              "          [ 19., 345., 211.,  ..., 439., 221.,  71.]],\n",
              "\n",
              "         [[433., 337., 493.,  ..., 195., 123., 353.],\n",
              "          [257., 515., 395.,  ..., 291., 149., 155.],\n",
              "          [419., 293., 331.,  ..., 223., 507., 337.],\n",
              "          ...,\n",
              "          [ 81., 237., 327.,  ..., 467., 297.,   9.],\n",
              "          [495., 397.,  35.,  ..., 507., 501., 277.],\n",
              "          [167., 395., 329.,  ...,  49., 515., 503.]]],\n",
              "\n",
              "\n",
              "        [[[503.,  23., 457.,  ..., 345., 465., 433.],\n",
              "          [ 39., 411., 487.,  ..., 185., 255., 105.],\n",
              "          [ 19., 285., 115.,  ..., 471., 219., 241.],\n",
              "          ...,\n",
              "          [357.,  41., 435.,  ..., 317., 135., 151.],\n",
              "          [ 37., 175., 333.,  ..., 391., 353., 105.],\n",
              "          [201., 161., 121.,  ..., 395., 395.,  51.]],\n",
              "\n",
              "         [[213.,  87., 165.,  ..., 223., 191., 139.],\n",
              "          [485., 329.,  15.,  ..., 409., 503., 137.],\n",
              "          [305., 377., 203.,  ..., 139., 409., 505.],\n",
              "          ...,\n",
              "          [421., 361., 375.,  ..., 273., 183., 383.],\n",
              "          [505., 419., 459.,  ..., 511., 355., 125.],\n",
              "          [115., 301., 497.,  ..., 207.,  81., 273.]],\n",
              "\n",
              "         [[125.,  93., 405.,  ..., 331.,  49., 499.],\n",
              "          [495.,  51., 211.,  ..., 509., 289., 191.],\n",
              "          [273., 365., 395.,  ..., 123., 205., 343.],\n",
              "          ...,\n",
              "          [205., 399., 327.,  ..., 257.,  87., 165.],\n",
              "          [249., 341., 279.,  ..., 329., 441., 179.],\n",
              "          [403., 151., 231.,  ..., 339., 103., 213.]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4fH_XLrImws"
      },
      "source": [
        "**2.ToTensor 를 사용하는 방향으로 가자. 즉 MyTransform()함수를 만들어서 PIL image 형태로 값을 받게 바꾸고, pytorch 지원함수들을 사용하자**\r\n",
        "** ***이 방법은 잘 안됨 ㅜㅜ*** **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua3KrxbRGtnt"
      },
      "source": [
        "class MyDataset(Dataset):\r\n",
        "\r\n",
        "  def __init__(self, x_data, y_data, transform=None):\r\n",
        "    # 텐서 변환 안함\r\n",
        "    self.x_data = x_data\r\n",
        "    self.y_data = y_data\r\n",
        "    self.transform = transform\r\n",
        "    self.len = len(y_data)\r\n",
        "    \r\n",
        "  def __getitem__(self, index): \r\n",
        "    # 튜플 형태로 내보내기 전에 전처리 작업 실행\r\n",
        "    sample = self.x_data[index], self.y_data[index]\r\n",
        "\r\n",
        "    if self.transform:\r\n",
        "      sample = self.transform(sample)\r\n",
        "\r\n",
        "    return sample   # 넘파이로 출력됨\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.len\r\n",
        "\r\n",
        "class MyTransform:\r\n",
        "\r\n",
        "  def __call__(self, sample):\r\n",
        "    inputs, labels = sample\r\n",
        "    inputs = torch.FloatTensor(inputs)\r\n",
        "    inputs = inputs.permute(2,0,1)\r\n",
        "    labels = torch.LongTensor(labels)\r\n",
        "\r\n",
        "    #transf = tr.Compose([tr.ToPILImage(), tr.Resize(128), tr.ToTensor(), tr.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5), (0.5,0.5,0.5))])\r\n",
        "    transf = tr.Compose([tr.ToPILImage(), tr.Resize(128), tr.ToTensor()])\r\n",
        "    final_output = transf(input)\r\n",
        "    \r\n",
        "    return final_output, labels"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq2WBFXDJtVk"
      },
      "source": [
        "ds2 = MyDataset(train_images, train_labels, transform=MyTransform())\r\n",
        "train_loader2 = DataLoader(ds2, batch_size=10, shuffle=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "sisAK9MiLhQG",
        "outputId": "0b41a85c-3736-48bf-83e5-1c7aa4f6d0b0"
      },
      "source": [
        "first_data = ds2[0]\r\n",
        "\r\n",
        "features, labels = first_data\r\n",
        "\r\n",
        "print(type(features), type(labels))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4d862d1a2661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfirst_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-e696bbe6247e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msample\u001b[0m   \u001b[0;31m# 넘파이로 출력됨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-e696bbe6247e>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#transf = tr.Compose([tr.ToPILImage(), tr.Resize(128), tr.ToTensor(), tr.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5), (0.5,0.5,0.5))])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtransf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \"\"\"\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pic should be Tensor or ndarray. Got {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: pic should be Tensor or ndarray. Got <class 'method'>."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUDtQyQ4LorC"
      },
      "source": [
        "dataiter2 = iter(train_loader2)\r\n",
        "images2, labels2 = dataiter2.next()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbykUdkyLsYP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}